# Split-Verse Architecture Documentation

## Overview

Split-Verse is a comprehensive sharding-aware repository framework for Java applications that provides transparent data partitioning and distribution across multiple MySQL databases. The framework supports both horizontal sharding (across multiple database servers) and vertical partitioning (time-based table partitioning within databases).

## Core Architecture

### 1. Generic Type System

Split-Verse uses a fully generic type system to support any `Comparable` type as the partition key:

```java
public interface ShardingEntity<T extends Comparable<? super T>> {
    String getId();
    void setId(String id);
    T getPartitionColValue();
    void setPartitionColValue(T value);
}
```

This allows entities to use:
- `LocalDateTime` for time-series data
- `Long` for sequence-based partitioning
- `String` for hash-based distribution
- Any other `Comparable` type

### 2. Repository Hierarchy

```
ShardingRepository<T, P>                 // Main interface
    └── SplitVerseRepository<T, P>       // Orchestrator for multiple shards
        ├── GenericMultiTableRepository  // Date-wise multi-table (daily/hourly/monthly)
        └── GenericPartitionedTableRepository  // Single table with MySQL partitions
```

### 3. Sharding Strategy

The framework supports multiple sharding strategies:

- **SINGLE_KEY_HASH**: Hash-based routing using entity ID
- **MULTI_KEY_HASH**: Multiple fields for hash computation
- **RANGE_BASED**: Range-based sharding for ordered data

### 4. Repository Modes

#### MULTI_TABLE Mode
Creates separate tables based on time granularity:
- Daily: `tablename_20250920`
- Hourly: `tablename_2025092014`
- Monthly: `tablename_202509`
- Yearly: `tablename_2025`

#### SINGLE_TABLE Mode
Uses MySQL native partitioning with a single table.

## Key Design Goals

### 1. Transparent Sharding
Applications work with entities without knowing about underlying data distribution. The framework handles:
- Automatic shard selection
- Cross-shard queries with parallel execution
- Failover to healthy shards

### 2. Automatic Partition Management
- **Table Creation**: Automatically creates tables for retention period on startup
- **Scheduled Maintenance**: Each shard runs its own scheduler for:
  - Creating future tables before they're needed
  - Dropping old tables beyond retention period
  - Maintenance runs at configurable time (e.g., 3 AM daily)

### 3. Performance Optimization
- **Connection Pooling**: Per-shard HikariCP connection pools
- **SQL Pre-generation**: Caches commonly used SQL statements
- **Batch Operations**: Optimized batch inserts grouped by target table
- **Parallel Queries**: Fan-out queries across shards with concurrent execution

### 4. Type Safety
Full compile-time type safety with Java generics:
```java
SplitVerseRepository<OrderEntity, LocalDateTime> orderRepo = ...
SplitVerseRepository<UserEntity, Long> userRepo = ...
```

### 5. Flexibility
- Pluggable persistence providers (MySQL, future: PostgreSQL)
- Configurable ID generation strategies
- Customizable table naming patterns
- Support for different partition ranges (10K, 100K, 1M records)

## API Reference

### Core Repository Methods

```java
public interface ShardingRepository<T extends ShardingEntity<P>, P extends Comparable<? super P>> {
    // Basic CRUD
    void insert(T entity) throws SQLException;
    void insertMultiple(List<T> entities) throws SQLException;
    T findById(String id) throws SQLException;
    void updateById(String id, T entity) throws SQLException;
    void deleteById(String id) throws SQLException;

    // Partition-aware queries
    List<T> findAllByPartitionRange(P startValue, P endValue) throws SQLException;
    T findByIdAndPartitionRange(String id, P start, P end) throws SQLException;
    List<T> findAllByIdsAndPartitionRange(List<String> ids, P start, P end) throws SQLException;

    // Pagination support
    T findNextById(String id) throws SQLException;
    List<T> findBatchByIdGreaterThan(String id, int batchSize) throws SQLException;

    // Lifecycle
    void shutdown();
}
```

### Builder Pattern Configuration

```java
SplitVerseRepository<EventEntity, LocalDateTime> repository =
    SplitVerseRepository.<EventEntity, LocalDateTime>builder()
        .withEntityClass(EventEntity.class)
        .withShardConfigs(shardConfigs)
        .withRepositoryMode(RepositoryMode.MULTI_TABLE)
        .withTableGranularity(TableGranularity.DAILY)
        .withRetentionDays(30)
        .withPartitionAdjustmentTime(LocalTime.of(3, 0))  // 3 AM maintenance
        .withSqlPregeneration()  // Enable SQL caching
        .build();
```

### Entity Definition

```java
@Table(name = "events")
public class EventEntity implements ShardingEntity<LocalDateTime> {
    @Id(autoGenerated = false)
    @Column(name = "event_id")
    private String id;

    @ShardingKey
    @Column(name = "created_at", nullable = false)
    private LocalDateTime createdAt;

    @Column(name = "event_type")
    private String eventType;

    // ShardingEntity implementation
    public LocalDateTime getPartitionColValue() { return createdAt; }
    public void setPartitionColValue(LocalDateTime value) { this.createdAt = value; }
}
```

## Implementation Details

### 1. Shard Routing
```
Client Request → SplitVerseRepository → HashRouter → Target Shard Repository
```

The HashRouter uses consistent hashing to ensure:
- Deterministic shard selection
- Even distribution of data
- Minimal data movement when adding/removing shards

### 2. Table Management Lifecycle

#### Startup Phase
1. Each shard's GenericMultiTableRepository initializes
2. Creates tables for retention period (past N days + future N days)
3. Schedules maintenance tasks if configured

#### Runtime Phase
1. Insert operations check/create tables as needed
2. Queries use table discovery to find relevant tables
3. Cross-table queries use UNION ALL for efficiency

#### Maintenance Phase (Daily)
1. Scheduler triggers at configured time (per shard)
2. Creates tables for next N days
3. Drops tables older than retention period
4. Uses exclusive maintenance connections to avoid conflicts

### 3. Query Execution

#### Single Shard Query
```sql
-- Direct query to specific table
SELECT * FROM database.events_20250920
WHERE created_at BETWEEN ? AND ?
```

#### Multi-Table Query (within shard)
```sql
-- UNION ALL across daily tables
(SELECT * FROM db.events_20250918 WHERE created_at BETWEEN ? AND ?)
UNION ALL
(SELECT * FROM db.events_20250919 WHERE created_at BETWEEN ? AND ?)
UNION ALL
(SELECT * FROM db.events_20250920 WHERE created_at BETWEEN ? AND ?)
```

#### Cross-Shard Query
- Parallel execution using CompletableFuture
- Results aggregation from all shards
- Automatic retry on transient failures

### 4. Connection Management

Each shard maintains:
- HikariCP connection pool (default 10 connections)
- Separate maintenance connection for DDL operations
- Health monitoring with automatic marking of failed shards

### 5. SQL Generation and Caching

The framework includes sophisticated SQL generation:
- EntityMetadata extracts field mappings via reflection
- SqlGeneratorByEntityRegistry pre-generates common patterns
- SqlStatementCache stores prepared statements by signature
- Batch sizes (1, 10, 100, 1000) are pre-generated at startup

## Configuration Properties

### Shard Configuration
```java
ShardConfig.builder()
    .shardId("shard-1")
    .host("127.0.0.1")
    .port(3306)
    .database("myapp_shard1")
    .username("app_user")
    .password("secret")
    .connectionPoolSize(20)
    .enabled(true)
    .build()
```

### Repository Configuration
- `repositoryMode`: MULTI_TABLE or SINGLE_TABLE
- `tableGranularity`: DAILY, HOURLY, MONTHLY, YEARLY
- `retentionDays`: How long to keep old partitions
- `partitionAdjustmentTime`: When to run maintenance (LocalTime)
- `autoCreatePartitions`: Auto-create tables on insert
- `initializePartitionsOnStart`: Pre-create tables at startup
- `partitionRange`: VALUE_RANGE_10K, VALUE_RANGE_100K, etc.

## Performance Characteristics

### Write Performance
- Single entity insert: ~1-2ms
- Batch insert (1000 entities): ~50-100ms
- Automatic batching by target table reduces round trips

### Read Performance
- Single entity by ID: ~1-2ms (single table scan)
- Date range query (30 days): ~20-50ms (parallel table scans)
- Cross-shard aggregation: Linear with shard count

### Storage Efficiency
- Daily tables enable efficient cleanup of old data
- No fragmentation from DELETE operations
- Optimal index usage per time period

## Monitoring and Operations

### Health Checks
- Per-shard connection validation
- Automatic failover on shard failure
- Health status exposed via monitoring service

### Metrics
- Query execution times
- Table creation/deletion counts
- Connection pool statistics
- Maintenance job success/failure rates

### Operational Tasks
- Manual table creation: `repository.ensureTableExists(date)`
- Force maintenance: `repository.performMaintenance()`
- Shard rebalancing: Add/remove shards with zero downtime

## Best Practices

### 1. Entity Design
- Use meaningful partition keys (usually timestamps)
- Keep partition column immutable after insert
- Generate IDs externally (UUID or Snowflake)

### 2. Query Patterns
- Always include partition key in WHERE clause when possible
- Use batch operations for bulk inserts
- Leverage partition pruning for range queries

### 3. Configuration
- Set retention period based on business requirements
- Schedule maintenance during low-traffic periods
- Size connection pools based on concurrent load

### 4. Deployment
- Start with single shard, scale horizontally as needed
- Monitor table counts and adjust retention accordingly
- Use read replicas for analytics queries

## Future Enhancements

1. **PostgreSQL Support**: Pluggable persistence provider
2. **Automatic Resharding**: Dynamic shard rebalancing
3. **Query Optimization**: Cost-based query planning
4. **Caching Layer**: Integrated Redis/Memcached support
5. **Async API**: Reactive programming model
6. **Metrics Integration**: Prometheus/Grafana dashboards

## Conclusion

Split-Verse provides a production-ready solution for handling high-volume, time-series data with automatic partitioning and sharding. Its design prioritizes:
- Developer productivity through transparent APIs
- Operational simplicity via automation
- Performance through intelligent data distribution
- Reliability with built-in failover mechanisms

The framework abstracts away the complexity of distributed data management while providing the flexibility needed for real-world applications.