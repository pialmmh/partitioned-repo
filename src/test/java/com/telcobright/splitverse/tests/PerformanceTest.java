package com.telcobright.splitverse.tests;

import com.telcobright.api.ShardingRepository;
import com.telcobright.core.config.DataSourceConfig;
import com.telcobright.core.entity.ShardingEntity;
import com.telcobright.core.enums.PartitionColumnType;
import com.telcobright.core.enums.PartitionRange;
import com.telcobright.core.enums.ShardingStrategy;
import com.telcobright.core.repository.SplitVerseRepository;
import com.telcobright.splitverse.config.RepositoryMode;
import com.telcobright.core.annotation.*;
import org.junit.jupiter.api.*;

import java.sql.SQLException;
import java.time.LocalDateTime;
import java.util.*;
import java.util.concurrent.*;

import static org.junit.jupiter.api.Assertions.*;

/**
 * Performance tests with large datasets (10K+ records).
 * Measures throughput, latency, and scalability.
 */
@TestMethodOrder(MethodOrderer.OrderAnnotation.class)
public class PerformanceTest {

    @Table(name = "events")
    public static class Event implements ShardingEntity {
        @Id(autoGenerated = false)
        @Column(name = "id")
        private String id;

        @ShardingKey
        @Column(name = "created_at")
        private LocalDateTime createdAt;

        @Column(name = "sequence_number")
        private Long sequenceNumber;

        @Column(name = "event_type")
        private String eventType;

        @Column(name = "payload")
        private String payload;

        @Column(name = "metric")
        private Double metric;

        @Override
        public String getId() { return id; }
        @Override
        public void setId(String id) { this.id = id; }
        @Override
        public LocalDateTime getCreatedAt() { return createdAt; }
        @Override
        public void setCreatedAt(LocalDateTime createdAt) { this.createdAt = createdAt; }

        public Long getSequenceNumber() { return sequenceNumber; }
        public void setSequenceNumber(Long sequenceNumber) { this.sequenceNumber = sequenceNumber; }
        public String getEventType() { return eventType; }
        public void setEventType(String eventType) { this.eventType = eventType; }
        public String getPayload() { return payload; }
        public void setPayload(String payload) { this.payload = payload; }
        public Double getMetric() { return metric; }
        public void setMetric(Double metric) { this.metric = metric; }
    }

    private static ShardingRepository<Event> repository;
    private static final String PERF_DB = "split_verse_perf_test";
    private static final int LARGE_DATASET_SIZE = 10000;
    private static final int VERY_LARGE_DATASET_SIZE = 50000;

    @BeforeAll
    public static void setup() throws SQLException {
        // Verify MySQL
        if (!TestDatabaseSetup.verifyMySQLConnection()) {
            fail("MySQL is not available");
        }

        // Create performance test database
        TestDatabaseSetup.createTestDatabase(PERF_DB);

        // Create repository with time-based partitioning for performance testing
        // NOTE: VALUE_RANGE partitioning has a bug - it still creates daily tables
        // Using DAILY partitioning instead with shorter retention for performance
        repository = SplitVerseRepository.<Event>builder()
            .withEntityClass(Event.class)
            .withTableName("events")
            .withShardingStrategy(ShardingStrategy.DUAL_KEY_HASH_RANGE)
            .withPartitionColumn("created_at", PartitionColumnType.LOCAL_DATE_TIME)
            .withPartitionRange(PartitionRange.DAILY)
            .withRepositoryMode(RepositoryMode.MULTI_TABLE)
            .withRetentionDays(3) // Only 3 days to reduce initialization time
            .withDataSources(Arrays.asList(
                DataSourceConfig.create("127.0.0.1", 3306, PERF_DB, "root", "123456")
            ))
            .build();

        System.out.println("Performance test setup complete");
    }

    @AfterAll
    public static void cleanup() {
        if (repository != null) {
            repository.shutdown();
        }
        TestDatabaseSetup.cleanupTestDatabases();
    }

    @Test
    @Order(1)
    public void testBulkInsertPerformance() throws SQLException {
        System.out.println("\n=== Bulk Insert Performance Test ===");

        // Prepare large dataset
        List<Event> events = generateEvents(LARGE_DATASET_SIZE);

        // Measure bulk insert time
        long startTime = System.currentTimeMillis();
        repository.insertMultiple(events);
        long duration = System.currentTimeMillis() - startTime;

        double throughput = (LARGE_DATASET_SIZE * 1000.0) / duration; // records per second
        double avgLatency = duration / (double) LARGE_DATASET_SIZE; // ms per record

        System.out.println("Inserted " + LARGE_DATASET_SIZE + " records in " + duration + "ms");
        System.out.println("Throughput: " + String.format("%.2f", throughput) + " records/sec");
        System.out.println("Avg latency: " + String.format("%.3f", avgLatency) + " ms/record");

        // Performance assertions
        assertTrue(throughput > 100, "Throughput should be > 100 records/sec");
        assertTrue(avgLatency < 10, "Average latency should be < 10ms per record");

        System.out.println("✓ Bulk insert performance test passed");
    }

    @Test
    @Order(2)
    public void testSequentialInsertPerformance() throws SQLException {
        System.out.println("\n=== Sequential Insert Performance Test ===");

        int testSize = 1000;
        long totalDuration = 0;
        List<Long> latencies = new ArrayList<>();

        for (int i = 0; i < testSize; i++) {
            Event event = generateEvent(i);

            long startTime = System.nanoTime();
            repository.insert(event);
            long duration = System.nanoTime() - startTime;

            totalDuration += duration;
            latencies.add(duration / 1_000_000); // Convert to ms
        }

        double avgLatency = (totalDuration / 1_000_000.0) / testSize;
        Collections.sort(latencies);
        long p50 = latencies.get(testSize / 2);
        long p95 = latencies.get((int)(testSize * 0.95));
        long p99 = latencies.get((int)(testSize * 0.99));

        System.out.println("Sequential insert statistics for " + testSize + " records:");
        System.out.println("Avg latency: " + String.format("%.2f", avgLatency) + " ms");
        System.out.println("P50 latency: " + p50 + " ms");
        System.out.println("P95 latency: " + p95 + " ms");
        System.out.println("P99 latency: " + p99 + " ms");

        // Performance assertions
        assertTrue(avgLatency < 50, "Average latency should be < 50ms");
        assertTrue(p95 < 100, "P95 latency should be < 100ms");

        System.out.println("✓ Sequential insert performance test passed");
    }

    @Test
    @Order(3)
    public void testQueryPerformance() throws SQLException {
        System.out.println("\n=== Query Performance Test ===");

        // Ensure we have data to query
        List<Event> events = generateEvents(5000);
        repository.insertMultiple(events);

        // Store some IDs for lookup
        List<String> sampleIds = events.stream()
            .limit(100)
            .map(Event::getId)
            .toList();

        // Test single record lookup performance
        long totalDuration = 0;
        for (String id : sampleIds) {
            long startTime = System.nanoTime();
            Event found = repository.findById(id);
            long duration = System.nanoTime() - startTime;
            totalDuration += duration;
            assertNotNull(found, "Should find event by ID");
        }

        double avgLookupTime = (totalDuration / 1_000_000.0) / sampleIds.size();
        System.out.println("Avg single record lookup: " + String.format("%.2f", avgLookupTime) + " ms");

        // Test range query performance
        LocalDateTime now = LocalDateTime.now();
        long startTime = System.currentTimeMillis();
        List<Event> rangeResults = repository.findAllByDateRange(
            now.minusHours(1),
            now.plusHours(1)
        );
        long rangeQueryTime = System.currentTimeMillis() - startTime;

        System.out.println("Range query returned " + rangeResults.size() +
            " records in " + rangeQueryTime + " ms");

        // Performance assertions
        assertTrue(avgLookupTime < 20, "Single record lookup should be < 20ms");
        assertTrue(rangeQueryTime < 5000, "Range query should complete in < 5 seconds");

        System.out.println("✓ Query performance test passed");
    }

    @Test
    @Order(4)
    public void testConcurrentOperationsPerformance() throws Exception {
        System.out.println("\n=== Concurrent Operations Performance Test ===");

        int threadCount = 20;
        int operationsPerThread = 500;
        ExecutorService executor = Executors.newFixedThreadPool(threadCount);
        CountDownLatch startLatch = new CountDownLatch(1);
        CountDownLatch completeLatch = new CountDownLatch(threadCount);

        List<Future<ThreadMetrics>> futures = new ArrayList<>();

        // Submit concurrent tasks
        for (int t = 0; t < threadCount; t++) {
            final int threadId = t;
            futures.add(executor.submit(() -> {
                ThreadMetrics metrics = new ThreadMetrics();

                try {
                    startLatch.await(); // Wait for signal to start
                    long threadStart = System.currentTimeMillis();

                    for (int i = 0; i < operationsPerThread; i++) {
                        // Mix of operations
                        Event event = generateEvent(threadId * 1000 + i);

                        // Insert
                        long opStart = System.nanoTime();
                        repository.insert(event);
                        metrics.insertLatencies.add(System.nanoTime() - opStart);

                        // Read
                        if (i % 5 == 0) {
                            opStart = System.nanoTime();
                            repository.findById(event.getId());
                            metrics.readLatencies.add(System.nanoTime() - opStart);
                        }

                        // Update
                        if (i % 10 == 0) {
                            event.setPayload("Updated_" + i);
                            opStart = System.nanoTime();
                            repository.updateById(event.getId(), event);
                            metrics.updateLatencies.add(System.nanoTime() - opStart);
                        }
                    }

                    metrics.totalDuration = System.currentTimeMillis() - threadStart;
                } finally {
                    completeLatch.countDown();
                }

                return metrics;
            }));
        }

        // Start all threads simultaneously
        long testStart = System.currentTimeMillis();
        startLatch.countDown();

        // Wait for completion
        completeLatch.await(60, TimeUnit.SECONDS);
        long totalTestDuration = System.currentTimeMillis() - testStart;

        // Collect metrics
        ThreadMetrics combined = new ThreadMetrics();
        for (Future<ThreadMetrics> future : futures) {
            ThreadMetrics metrics = future.get();
            combined.merge(metrics);
        }

        executor.shutdown();

        // Calculate statistics
        double totalOps = threadCount * operationsPerThread;
        double overallThroughput = (totalOps * 1000.0) / totalTestDuration;

        System.out.println("Concurrent test completed in " + totalTestDuration + " ms");
        System.out.println("Total operations: " + (int)totalOps);
        System.out.println("Overall throughput: " + String.format("%.2f", overallThroughput) + " ops/sec");
        System.out.println("Insert P50: " + combined.getP50(combined.insertLatencies) / 1_000_000 + " ms");
        System.out.println("Read P50: " + combined.getP50(combined.readLatencies) / 1_000_000 + " ms");
        System.out.println("Update P50: " + combined.getP50(combined.updateLatencies) / 1_000_000 + " ms");

        // Performance assertions
        assertTrue(overallThroughput > 50, "Concurrent throughput should be > 50 ops/sec");

        System.out.println("✓ Concurrent operations performance test passed");
    }

    @Test
    @Order(5)
    public void testLargeDatasetHandling() throws SQLException {
        System.out.println("\n=== Large Dataset Handling Test ===");

        // Insert very large dataset to test partition management
        System.out.println("Inserting " + VERY_LARGE_DATASET_SIZE + " records...");

        int batchSize = 5000;
        long totalInsertTime = 0;

        for (int batch = 0; batch < VERY_LARGE_DATASET_SIZE / batchSize; batch++) {
            List<Event> events = new ArrayList<>();
            for (int i = 0; i < batchSize; i++) {
                Event event = generateEvent(batch * batchSize + i);
                event.setSequenceNumber((long)(batch * batchSize + i)); // Ensure different partitions
                events.add(event);
            }

            long startTime = System.currentTimeMillis();
            repository.insertMultiple(events);
            totalInsertTime += System.currentTimeMillis() - startTime;

            if ((batch + 1) % 2 == 0) {
                System.out.println("Progress: " + ((batch + 1) * batchSize) + "/" + VERY_LARGE_DATASET_SIZE);
            }
        }

        double avgBatchTime = totalInsertTime / (double)(VERY_LARGE_DATASET_SIZE / batchSize);
        System.out.println("Total insert time: " + totalInsertTime + " ms");
        System.out.println("Avg batch insert time: " + String.format("%.2f", avgBatchTime) + " ms");

        // Test querying large dataset
        long queryStart = System.currentTimeMillis();
        List<Event> sample = repository.findBatchByIdGreaterThan("", 1000);
        long queryTime = System.currentTimeMillis() - queryStart;

        System.out.println("Retrieved " + sample.size() + " records from large dataset in " + queryTime + " ms");

        // Verify partitioning is working (should have created multiple tables)
        // With DAILY partitioning, we have tables for different days
        assertTrue(sample.size() > 0, "Should retrieve records from partitioned tables");

        System.out.println("✓ Large dataset handling test passed");
    }

    @Test
    @Order(6)
    public void testPaginationPerformance() throws SQLException {
        System.out.println("\n=== Pagination Performance Test ===");

        // Ensure we have enough data
        if (repository.findOneByIdGreaterThan("") == null) {
            List<Event> events = generateEvents(5000);
            repository.insertMultiple(events);
        }

        // Test cursor-based pagination performance
        String cursor = "";
        int pageSize = 100;
        int pageCount = 0;
        long totalPaginationTime = 0;
        List<Long> pageLatencies = new ArrayList<>();

        while (pageCount < 50) { // Fetch 50 pages
            long startTime = System.currentTimeMillis();
            List<Event> page = repository.findBatchByIdGreaterThan(cursor, pageSize);
            long duration = System.currentTimeMillis() - startTime;

            if (page.isEmpty()) break;

            pageLatencies.add(duration);
            totalPaginationTime += duration;
            cursor = page.get(page.size() - 1).getId();
            pageCount++;
        }

        double avgPageLatency = totalPaginationTime / (double) pageCount;
        Collections.sort(pageLatencies);
        long p50Page = pageLatencies.get(pageCount / 2);
        long p95Page = pageLatencies.get(Math.min((int)(pageCount * 0.95), pageLatencies.size() - 1));

        System.out.println("Pagination statistics for " + pageCount + " pages:");
        System.out.println("Avg page fetch time: " + String.format("%.2f", avgPageLatency) + " ms");
        System.out.println("P50 page fetch time: " + p50Page + " ms");
        System.out.println("P95 page fetch time: " + p95Page + " ms");
        System.out.println("Total records paginated: " + (pageCount * pageSize));

        // Performance assertions
        assertTrue(avgPageLatency < 100, "Average page fetch should be < 100ms");
        assertTrue(p95Page < 200, "P95 page fetch should be < 200ms");

        System.out.println("✓ Pagination performance test passed");
    }

    // Helper methods

    private Event generateEvent(int index) {
        Event event = new Event();
        event.setId(String.format("EVT_%08d_%s", index,
            UUID.randomUUID().toString().substring(0, 8)));
        event.setCreatedAt(LocalDateTime.now().plusSeconds(index % 3600));
        event.setSequenceNumber((long) index);
        event.setEventType("TYPE_" + (index % 10));
        event.setPayload("Payload data for event " + index + " with some additional content");
        event.setMetric(Math.random() * 1000);
        return event;
    }

    private List<Event> generateEvents(int count) {
        List<Event> events = new ArrayList<>(count);
        for (int i = 0; i < count; i++) {
            events.add(generateEvent(i));
        }
        return events;
    }

    // Metrics collection class
    private static class ThreadMetrics {
        List<Long> insertLatencies = new ArrayList<>();
        List<Long> readLatencies = new ArrayList<>();
        List<Long> updateLatencies = new ArrayList<>();
        long totalDuration;

        void merge(ThreadMetrics other) {
            insertLatencies.addAll(other.insertLatencies);
            readLatencies.addAll(other.readLatencies);
            updateLatencies.addAll(other.updateLatencies);
        }

        long getP50(List<Long> latencies) {
            if (latencies.isEmpty()) return 0;
            Collections.sort(latencies);
            return latencies.get(latencies.size() / 2);
        }
    }
}