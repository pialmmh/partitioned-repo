package com.telcobright.splitverse.tests;

import com.telcobright.api.ShardingRepository;
import com.telcobright.core.config.DataSourceConfig;
import com.telcobright.core.entity.ShardingEntity;
import com.telcobright.core.enums.PartitionColumnType;
import com.telcobright.core.enums.PartitionRange;
import com.telcobright.core.enums.ShardingStrategy;
import com.telcobright.core.repository.SplitVerseRepository;
import com.telcobright.splitverse.config.RepositoryMode;
import com.telcobright.core.annotation.*;
import org.junit.jupiter.api.*;

import java.sql.SQLException;
import java.time.LocalDateTime;
import java.util.*;
import java.util.concurrent.*;
import java.util.stream.Collectors;

import static org.junit.jupiter.api.Assertions.*;

/**
 * Comprehensive test for 3-database sharding with nested level partitioning.
 * Tests multi-table sharding with insert, select, update, and delete operations.
 *
 * Architecture:
 * - Level 1: 3 Database Shards (based on ID hash)
 * - Level 2: Multi-table partitioning within each shard (based on time/value)
 *
 * Scenarios tested:
 * 1. Time-based nested partitioning (daily tables within each shard)
 * 2. Value-based nested partitioning (range tables within each shard)
 * 3. Hash-based nested partitioning (hash buckets within each shard)
 */
@TestMethodOrder(MethodOrderer.OrderAnnotation.class)
public class ThreeShardNestedPartitioningTest {

    // ==================== Test Entities ====================

    @Table(name = "transactions")
    public static class Transaction implements ShardingEntity<LocalDateTime> {
        @Id(autoGenerated = false)
        @Column(name = "id")
        private String id;

        @ShardingKey
        @Column(name = "created_at")
        private LocalDateTime createdAt;

        @Column(name = "account_id")
        private String accountId;

        @Column(name = "amount")
        private Double amount;

        @Column(name = "sequence_number")
        private Long sequenceNumber;

        @Column(name = "type")
        private String type;

        @Column(name = "status")
        private String status;

        @Column(name = "metadata")
        private String metadata;

        @Override
        public String getId() { return id; }
        @Override
        public void setId(String id) { this.id = id; }
        @Override
        public LocalDateTime getCreatedAt() { return createdAt; }
        @Override
        public void setCreatedAt(LocalDateTime createdAt) { this.createdAt = createdAt; }

        public String getAccountId() { return accountId; }
        public void setAccountId(String accountId) { this.accountId = accountId; }
        public Double getAmount() { return amount; }
        public void setAmount(Double amount) { this.amount = amount; }
        public Long getSequenceNumber() { return sequenceNumber; }
        public void setSequenceNumber(Long sequenceNumber) { this.sequenceNumber = sequenceNumber; }
        public String getType() { return type; }
        public void setType(String type) { this.type = type; }
        public String getStatus() { return status; }
        public void setStatus(String status) { this.status = status; }
        public String getMetadata() { return metadata; }
        public void setMetadata(String metadata) { this.metadata = metadata; }
    }

    // ==================== Test Configuration ====================

    private static final int SHARD_COUNT = 3;
    private static final String DB_PREFIX = "shard_nested_test_";
    private static List<String> testDatabases;

    // Repositories for different partitioning strategies
    private static ShardingRepository<Transaction, LocalDateTime> timeBasedRepo;
    private static ShardingRepository<Transaction, LocalDateTime> valueBasedRepo;
    private static ShardingRepository<Transaction, LocalDateTime> hashBasedRepo;

    // Track inserted data for verification
    private static Map<String, List<Transaction>> insertedByRepo = new HashMap<>();

    @BeforeAll
    public static void setupTestEnvironment() throws SQLException {
        System.out.println("\n=== Setting up 3-Shard Nested Partitioning Test Environment ===");

        // Verify MySQL
        if (!TestDatabaseSetup.verifyMySQLConnection()) {
            fail("MySQL is not available at 127.0.0.1:3306");
        }

        // Create 3 test databases for sharding
        testDatabases = new ArrayList<>();
        for (int i = 0; i < SHARD_COUNT; i++) {
            String dbName = DB_PREFIX + i;
            TestDatabaseSetup.createTestDatabase(dbName);
            testDatabases.add(dbName);
            System.out.println("Created shard database: " + dbName);
        }

        // Setup data sources for 3 shards
        List<DataSourceConfig> dataSources = testDatabases.stream()
            .map(db -> DataSourceConfig.create("127.0.0.1", 3306, db, "root", "123456"))
            .collect(Collectors.toList());

        // Create repository with TIME-BASED nested partitioning (Daily tables within each shard)
        System.out.println("\nCreating TIME-BASED nested partitioning repository...");
        timeBasedRepo = SplitVerseRepository.<Transaction, LocalDateTime>builder()
            .withEntityClass(Transaction.class)
            .withTableName("transactions_time")
            .withShardingStrategy(ShardingStrategy.DUAL_KEY_HASH_RANGE)
            .withPartitionColumn("created_at", PartitionColumnType.LOCAL_DATE_TIME)
            .withPartitionRange(PartitionRange.DAILY)
            .withRepositoryMode(RepositoryMode.MULTI_TABLE)
            .withRetentionDays(7)
            .withDataSources(dataSources)
            .build();

        // Create repository with VALUE-BASED nested partitioning (10K range tables within each shard)
        System.out.println("Creating VALUE-BASED nested partitioning repository...");
        valueBasedRepo = SplitVerseRepository.<Transaction, LocalDateTime>builder()
            .withEntityClass(Transaction.class)
            .withTableName("transactions_value")
            .withShardingStrategy(ShardingStrategy.DUAL_KEY_HASH_RANGE)
            .withPartitionColumn("sequence_number", PartitionColumnType.LONG)
            .withPartitionRange(PartitionRange.VALUE_RANGE_10K)
            .withRepositoryMode(RepositoryMode.MULTI_TABLE)
            .withDataSources(dataSources)
            .build();

        // Create repository with HASH-BASED nested partitioning (16 hash buckets within each shard)
        System.out.println("Creating HASH-BASED nested partitioning repository...");
        hashBasedRepo = SplitVerseRepository.<Transaction, LocalDateTime>builder()
            .withEntityClass(Transaction.class)
            .withTableName("transactions_hash")
            .withShardingStrategy(ShardingStrategy.DUAL_KEY_HASH_HASH)
            .withPartitionColumn("account_id", PartitionColumnType.STRING)
            .withPartitionRange(PartitionRange.HASH_16)
            .withRepositoryMode(RepositoryMode.MULTI_TABLE)
            .withDataSources(dataSources)
            .build();

        System.out.println("✓ Test environment setup complete with 3 shards and 3 partitioning strategies\n");
    }

    @AfterAll
    public static void teardownTestEnvironment() {
        System.out.println("\n=== Cleaning up test environment ===");

        // Shutdown repositories
        if (timeBasedRepo != null) timeBasedRepo.shutdown();
        if (valueBasedRepo != null) valueBasedRepo.shutdown();
        if (hashBasedRepo != null) hashBasedRepo.shutdown();

        // Drop test databases
        TestDatabaseSetup.cleanupTestDatabases();

        System.out.println("✓ Cleanup complete");
    }

    // ==================== Test 1: INSERT Operations ====================

    @Test
    @Order(1)
    public void testInsertAcross3ShardsWithNestedPartitions() throws SQLException {
        System.out.println("\n=== Test 1: INSERT across 3 shards with nested partitions ===");

        int recordsPerStrategy = 300; // 100 records per shard

        // Generate test data with various characteristics
        List<Transaction> transactions = generateTransactions(recordsPerStrategy);

        // Test 1.1: Insert into TIME-BASED partitioned repository
        System.out.println("\n1.1 Inserting into TIME-BASED partitioned tables...");
        long startTime = System.currentTimeMillis();

        for (Transaction tx : transactions) {
            timeBasedRepo.insert(tx);
        }

        long timeDuration = System.currentTimeMillis() - startTime;
        System.out.println("Inserted " + recordsPerStrategy + " records across 3 shards with daily partitions in " + timeDuration + "ms");
        insertedByRepo.put("time", new ArrayList<>(transactions));

        // Test 1.2: Insert into VALUE-BASED partitioned repository
        System.out.println("\n1.2 Inserting into VALUE-BASED partitioned tables...");
        List<Transaction> valueTransactions = generateTransactions(recordsPerStrategy);
        startTime = System.currentTimeMillis();

        // Batch insert
        valueBasedRepo.insertMultiple(valueTransactions);

        long valueDuration = System.currentTimeMillis() - startTime;
        System.out.println("Batch inserted " + recordsPerStrategy + " records across 3 shards with value partitions in " + valueDuration + "ms");
        insertedByRepo.put("value", valueTransactions);

        // Test 1.3: Insert into HASH-BASED partitioned repository
        System.out.println("\n1.3 Inserting into HASH-BASED partitioned tables...");
        List<Transaction> hashTransactions = generateTransactions(recordsPerStrategy);
        startTime = System.currentTimeMillis();

        // Concurrent inserts to test thread safety
        ExecutorService executor = Executors.newFixedThreadPool(10);
        List<Future<Boolean>> futures = new ArrayList<>();

        for (Transaction tx : hashTransactions) {
            futures.add(executor.submit(() -> {
                try {
                    hashBasedRepo.insert(tx);
                    return true;
                } catch (SQLException e) {
                    System.err.println("Insert failed: " + e.getMessage());
                    return false;
                }
            }));
        }

        // Wait for all inserts to complete
        int successCount = 0;
        for (Future<Boolean> future : futures) {
            try {
                if (future.get()) successCount++;
            } catch (Exception e) {
                System.err.println("Future failed: " + e.getMessage());
            }
        }

        executor.shutdown();
        try {
            executor.awaitTermination(10, TimeUnit.SECONDS);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }

        long hashDuration = System.currentTimeMillis() - startTime;
        System.out.println("Concurrently inserted " + successCount + "/" + recordsPerStrategy +
            " records across 3 shards with hash partitions in " + hashDuration + "ms");
        insertedByRepo.put("hash", hashTransactions);

        // Verify distribution across shards
        System.out.println("\n✓ INSERT test complete. Data distributed across:");
        System.out.println("  - 3 database shards");
        System.out.println("  - Multiple partition tables within each shard");
        System.out.println("  - Total records: " + (recordsPerStrategy * 3));
    }

    // ==================== Test 2: SELECT Operations ====================

    @Test
    @Order(2)
    public void testSelectAcross3ShardsWithNestedPartitions() throws SQLException {
        System.out.println("\n=== Test 2: SELECT across 3 shards with nested partitions ===");

        // Test 2.1: Point queries (by ID)
        System.out.println("\n2.1 Testing point queries across shards...");
        List<Transaction> timeData = insertedByRepo.get("time");
        if (timeData != null && !timeData.isEmpty()) {
            // Select random IDs to query
            Random random = new Random();
            int queriesToTest = 30;
            int found = 0;

            long startTime = System.currentTimeMillis();
            for (int i = 0; i < queriesToTest; i++) {
                Transaction original = timeData.get(random.nextInt(timeData.size()));
                Transaction fetched = timeBasedRepo.findById(original.getId());
                if (fetched != null && fetched.getId().equals(original.getId())) {
                    found++;
                }
            }
            long duration = System.currentTimeMillis() - startTime;

            assertEquals(queriesToTest, found, "All point queries should find their records");
            System.out.println("✓ Point queries: " + found + "/" + queriesToTest +
                " records found in " + duration + "ms");
        }

        // Test 2.2: Range queries (by date)
        System.out.println("\n2.2 Testing date range queries across partitions...");
        LocalDateTime now = LocalDateTime.now();
        List<Transaction> rangeResults = timeBasedRepo.findAllByDateRange(
            now.minusDays(3),
            now.plusDays(3)
        );

        assertTrue(rangeResults.size() > 0, "Range query should return results");
        System.out.println("✓ Date range query returned " + rangeResults.size() +
            " records from multiple partitions across 3 shards");

        // Test 2.3: Pagination across shards
        System.out.println("\n2.3 Testing pagination across shards...");
        String cursor = "";
        int pageSize = 50;
        int totalPages = 0;
        int totalRecords = 0;
        Set<String> uniqueIds = new HashSet<>();

        while (totalPages < 10) { // Limit to 10 pages for test
            List<Transaction> page = valueBasedRepo.findBatchByIdGreaterThan(cursor, pageSize);
            if (page.isEmpty()) break;

            totalPages++;
            totalRecords += page.size();

            // Verify no duplicates
            for (Transaction tx : page) {
                assertTrue(uniqueIds.add(tx.getId()), "No duplicate IDs in pagination");
            }

            cursor = page.get(page.size() - 1).getId();
        }

        System.out.println("✓ Paginated through " + totalPages + " pages, " +
            totalRecords + " unique records");

        // Test 2.4: Complex queries (combining conditions)
        System.out.println("\n2.4 Testing complex queries...");
        List<String> specificIds = insertedByRepo.get("hash").stream()
            .limit(10)
            .map(Transaction::getId)
            .collect(Collectors.toList());

        List<Transaction> complexResults = hashBasedRepo.findAllByIdsAndDateRange(
            specificIds,
            now.minusHours(12),
            now.plusHours(12)
        );

        assertTrue(complexResults.size() <= specificIds.size(),
            "Complex query results should be bounded by ID list");
        System.out.println("✓ Complex query (IDs + date range) returned " +
            complexResults.size() + " records");
    }

    // ==================== Test 3: UPDATE Operations ====================

    @Test
    @Order(3)
    public void testUpdateAcross3ShardsWithNestedPartitions() throws SQLException {
        System.out.println("\n=== Test 3: UPDATE across 3 shards with nested partitions ===");

        // Test 3.1: Single record updates
        System.out.println("\n3.1 Testing single record updates...");
        List<Transaction> timeData = insertedByRepo.get("time");
        if (timeData != null && !timeData.isEmpty()) {
            int updateCount = 0;
            int samplesToUpdate = 30;

            for (int i = 0; i < samplesToUpdate && i < timeData.size(); i++) {
                Transaction original = timeData.get(i);

                // Fetch current state
                Transaction current = timeBasedRepo.findById(original.getId());
                assertNotNull(current, "Record should exist before update");

                // Update fields
                current.setStatus("UPDATED");
                current.setAmount(current.getAmount() * 1.1); // 10% increase
                current.setMetadata("Updated at: " + LocalDateTime.now());

                // Perform update
                timeBasedRepo.updateById(current.getId(), current);
                updateCount++;

                // Verify update
                Transaction updated = timeBasedRepo.findById(current.getId());
                assertEquals("UPDATED", updated.getStatus());
                assertEquals(current.getAmount(), updated.getAmount(), 0.01);
            }

            System.out.println("✓ Successfully updated " + updateCount +
                " records across shards and partitions");
        }

        // Test 3.2: Batch updates within same partition
        System.out.println("\n3.2 Testing batch updates within partitions...");
        List<Transaction> valueData = insertedByRepo.get("value");
        if (valueData != null && !valueData.isEmpty()) {
            // Group by sequence number range (same partition)
            Map<Long, List<Transaction>> byPartition = valueData.stream()
                .collect(Collectors.groupingBy(
                    tx -> tx.getSequenceNumber() / 10000 // VALUE_RANGE_10K
                ));

            int totalUpdated = 0;
            for (Map.Entry<Long, List<Transaction>> entry : byPartition.entrySet()) {
                List<Transaction> partitionData = entry.getValue();
                if (partitionData.size() > 5) {
                    // Update first 5 records in this partition
                    for (int i = 0; i < 5; i++) {
                        Transaction tx = partitionData.get(i);
                        tx.setType("BATCH_UPDATED");
                        valueBasedRepo.updateById(tx.getId(), tx);
                        totalUpdated++;
                    }
                }
            }

            System.out.println("✓ Batch updated " + totalUpdated +
                " records grouped by partition");
        }

        // Test 3.3: Concurrent updates across shards
        System.out.println("\n3.3 Testing concurrent updates across shards...");
        List<Transaction> hashData = insertedByRepo.get("hash");
        if (hashData != null && !hashData.isEmpty()) {
            ExecutorService executor = Executors.newFixedThreadPool(10);
            List<Future<Boolean>> futures = new ArrayList<>();

            int concurrentUpdates = Math.min(50, hashData.size());
            for (int i = 0; i < concurrentUpdates; i++) {
                final Transaction tx = hashData.get(i);
                futures.add(executor.submit(() -> {
                    try {
                        tx.setStatus("CONCURRENT_UPDATE");
                        tx.setMetadata("Thread: " + Thread.currentThread().getName());
                        hashBasedRepo.updateById(tx.getId(), tx);
                        return true;
                    } catch (SQLException e) {
                        return false;
                    }
                }));
            }

            int successCount = 0;
            for (Future<Boolean> future : futures) {
                try {
                    if (future.get()) successCount++;
                } catch (Exception e) {
                    System.err.println("Update failed: " + e.getMessage());
                }
            }

            executor.shutdown();
            try {
                executor.awaitTermination(10, TimeUnit.SECONDS);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            }

            System.out.println("✓ Concurrent updates: " + successCount + "/" +
                concurrentUpdates + " successful");
        }
    }

    // ==================== Test 4: DELETE Operations ====================

    @Test
    @Order(4)
    public void testDeleteAcross3ShardsWithNestedPartitions() throws SQLException {
        System.out.println("\n=== Test 4: DELETE across 3 shards with nested partitions ===");

        // Test 4.1: Single record deletes
        System.out.println("\n4.1 Testing single record deletes...");
        List<Transaction> timeData = insertedByRepo.get("time");
        if (timeData != null && !timeData.isEmpty()) {
            int deleteCount = 10;
            List<String> toDelete = timeData.stream()
                .limit(deleteCount)
                .map(Transaction::getId)
                .collect(Collectors.toList());

            for (String id : toDelete) {
                timeBasedRepo.deleteById(id);
            }

            // Verify deletion
            for (String id : toDelete) {
                assertNull(timeBasedRepo.findById(id),
                    "Deleted record should not be found");
            }

            System.out.println("✓ Successfully deleted " + deleteCount +
                " individual records");
        }

        // Test 4.2: Range-based deletes (by date)
        System.out.println("\n4.2 Testing range-based deletes...");
        LocalDateTime now = LocalDateTime.now();
        LocalDateTime deleteStart = now.minusHours(1);
        LocalDateTime deleteEnd = now.plusMinutes(30);

        // Count records before deletion
        List<Transaction> beforeDelete = timeBasedRepo.findAllByDateRange(
            deleteStart, deleteEnd);
        int recordsBeforeDelete = beforeDelete.size();

        if (recordsBeforeDelete > 0) {
            // Perform range delete
            timeBasedRepo.deleteAllByDateRange(deleteStart, deleteEnd);

            // Verify deletion
            List<Transaction> afterDelete = timeBasedRepo.findAllByDateRange(
                deleteStart, deleteEnd);

            System.out.println("✓ Range delete removed " +
                (recordsBeforeDelete - afterDelete.size()) + " records");
        }

        // Test 4.3: Delete with ID and date range combination
        System.out.println("\n4.3 Testing combined ID and date range deletes...");
        List<Transaction> valueData = insertedByRepo.get("value");
        if (valueData != null && !valueData.isEmpty()) {
            // Select specific record
            Transaction target = valueData.get(valueData.size() / 2);

            valueBasedRepo.deleteByIdAndDateRange(
                target.getId(),
                target.getCreatedAt().minusMinutes(1),
                target.getCreatedAt().plusMinutes(1)
            );

            // Verify deletion
            assertNull(valueBasedRepo.findById(target.getId()),
                "Record should be deleted by ID and date range");

            System.out.println("✓ Combined delete (ID + date range) successful");
        }

        // Test 4.4: Concurrent deletes
        System.out.println("\n4.4 Testing concurrent deletes across shards...");
        List<Transaction> hashData = insertedByRepo.get("hash");
        if (hashData != null && hashData.size() > 20) {
            ExecutorService executor = Executors.newFixedThreadPool(5);
            List<Future<Boolean>> futures = new ArrayList<>();

            // Delete last 20 records concurrently
            for (int i = hashData.size() - 20; i < hashData.size(); i++) {
                final String deleteId = hashData.get(i).getId();
                futures.add(executor.submit(() -> {
                    try {
                        hashBasedRepo.deleteById(deleteId);
                        return true;
                    } catch (SQLException e) {
                        return false;
                    }
                }));
            }

            int successCount = 0;
            for (Future<Boolean> future : futures) {
                try {
                    if (future.get()) successCount++;
                } catch (Exception e) {
                    System.err.println("Delete failed: " + e.getMessage());
                }
            }

            executor.shutdown();
            try {
                executor.awaitTermination(10, TimeUnit.SECONDS);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            }

            System.out.println("✓ Concurrent deletes: " + successCount + "/20 successful");
        }
    }

    // ==================== Test 5: Verification and Statistics ====================

    @Test
    @Order(5)
    public void testVerifyDistributionAndPerformance() throws SQLException {
        System.out.println("\n=== Test 5: Verify distribution and performance ===");

        // Collect statistics for each repository
        System.out.println("\n5.1 Data distribution statistics:");

        // Time-based repository stats
        List<Transaction> remainingTime = timeBasedRepo.findAllByDateRange(
            LocalDateTime.now().minusDays(10),
            LocalDateTime.now().plusDays(10)
        );
        System.out.println("  TIME-BASED: " + remainingTime.size() + " records remaining");

        // Value-based repository stats
        int valueCount = 0;
        String cursor = "";
        while (valueCount < 1000) { // Sample up to 1000 records
            List<Transaction> batch = valueBasedRepo.findBatchByIdGreaterThan(cursor, 100);
            if (batch.isEmpty()) break;
            valueCount += batch.size();
            cursor = batch.get(batch.size() - 1).getId();
        }
        System.out.println("  VALUE-BASED: " + valueCount + "+ records accessible");

        // Hash-based repository stats
        List<Transaction> hashSample = hashBasedRepo.findBatchByIdGreaterThan("", 500);
        System.out.println("  HASH-BASED: " + hashSample.size() + "+ records in sample");

        // Performance summary
        System.out.println("\n5.2 Performance characteristics:");
        System.out.println("  ✓ 3 database shards utilized");
        System.out.println("  ✓ Nested partitioning within each shard");
        System.out.println("  ✓ Time-based: Daily partition tables");
        System.out.println("  ✓ Value-based: 10K range partition tables");
        System.out.println("  ✓ Hash-based: 16 hash bucket tables");
        System.out.println("  ✓ All CRUD operations verified");

        // Shard distribution verification
        System.out.println("\n5.3 Shard distribution:");
        Map<Integer, Integer> shardDistribution = new HashMap<>();
        for (Transaction tx : hashSample) {
            int shardIndex = Math.abs(tx.getId().hashCode()) % SHARD_COUNT;
            shardDistribution.merge(shardIndex, 1, Integer::sum);
        }

        for (Map.Entry<Integer, Integer> entry : shardDistribution.entrySet()) {
            System.out.println("  Shard " + entry.getKey() + ": ~" +
                (entry.getValue() * 100 / hashSample.size()) + "% of data");
        }

        // Verify even distribution
        int expectedPerShard = hashSample.size() / SHARD_COUNT;
        for (int count : shardDistribution.values()) {
            assertTrue(count > expectedPerShard * 0.7 && count < expectedPerShard * 1.3,
                "Data should be relatively evenly distributed (±30%)");
        }

        System.out.println("\n✓ All tests completed successfully!");
    }

    // ==================== Helper Methods ====================

    private List<Transaction> generateTransactions(int count) {
        List<Transaction> transactions = new ArrayList<>();
        LocalDateTime baseTime = LocalDateTime.now().minusDays(3);

        for (int i = 0; i < count; i++) {
            Transaction tx = new Transaction();
            tx.setId(generateId());
            tx.setCreatedAt(baseTime.plusMinutes(i * 5)); // Spread across time
            tx.setAccountId("ACC_" + (i % 100)); // 100 different accounts
            tx.setAmount(100.0 + (Math.random() * 9900)); // 100-10000
            tx.setSequenceNumber((long)(i * 100)); // Spread across value ranges
            tx.setType(i % 3 == 0 ? "CREDIT" : i % 3 == 1 ? "DEBIT" : "TRANSFER");
            tx.setStatus("PENDING");
            tx.setMetadata("Test transaction " + i);
            transactions.add(tx);
        }

        return transactions;
    }

    private String generateId() {
        return UUID.randomUUID().toString().replace("-", "").substring(0, 22);
    }
}